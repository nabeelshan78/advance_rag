{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbfe888-a5c8-4b2f-a9f4-7be92dd8ae64",
   "metadata": {},
   "source": [
    "# Exploring LLM Context & Generation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Build utility functions for structured LLM calls.\n",
    "- Maintain conversation context across multiple turns.\n",
    "- Experiment with generation parameters (`temperature`, `top_p`, `top_k`, `repetition_penalty`).\n",
    "- Observe how these settings influence creativity, coherence, and variability in responses.\n",
    "\n",
    "Utilities are provided in **`utils.py`**, while this notebook walks through examples, experiments, and chatbot-like interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a71245a-5dbf-4289-914b-17881d36a577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf32a19-e18b-4efd-840f-19233fda13e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    generate_with_single_input, \n",
    "    generate_with_multiple_input,\n",
    "    generate_params_dict,\n",
    "    call_llm_with_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdc8da-6310-434e-9e2a-587f50ebb55b",
   "metadata": {},
   "source": [
    "## Generation functions\n",
    "\n",
    "\n",
    "### `generate_with_single_input` and `generate_with_multiple_input`\n",
    "\n",
    "```Python\n",
    "generate_with_single_input(prompt: str, \n",
    "                               role: str = 'user', \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "\n",
    "\n",
    "generate_with_multiple_input(messages: List[Dict], \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ab96cff-654a-4d82-abf3-84a7f1d3bbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'A Cyclotomic Polynomial is a polynomial that arises from the roots of unity, specifically the nth roots of unity. It is defined as the product of linear factors of the form (x - ω), where ω is a primitive nth root of unity. The coefficients of the polynomial are integers, and it has a specific structure that relates to the properties of roots of unity. Cyclotomic Polynomials have numerous applications in number theory, algebra, and computer science. They are named after the Greek word \"kyklotomos,\" meaning \"circular,\" due to their connection to the roots of unity.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_single_input(\"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8be225-d6f8-4123-9b3a-2c0f813b374d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function `generate_with_multiple_input` inputs a list of messages with the format `{'role': role, 'content': prompt}`. This function allows us to **create context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d8138cf-447a-4527-82e5-fe0ade5c343a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"What a delightfully complex topic. In essence, the Cyclotomic Polynomial is a polynomial that arises from the roots of unity, specifically the complex numbers that satisfy a polynomial equation with integer coefficients. Imagine you're trying to quantify the nth roots of unity, a Cyclotomic Polynomial is one way to formalize that. They possess interesting algebraic and number-theoretic properties, often making them a fascinating area of study. It's like a recurring pattern in the world of math.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_dict = {\"role\": 'system', 'content': 'You are a very ironic, but helpful assistant.'}\n",
    "user_dict = {\"role\":\"user\", 'content': \"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\"}\n",
    "messages = [system_dict, user_dict]\n",
    "generate_with_multiple_input(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3febd3a-e65a-4626-ad28-ce2e2f751ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"In moonlit fields of silver bright,\\nA rabbit takes to the starry night,\\nWith wings of silk and eyes of gleam,\\nShe soars on wind, a wondrous dream.\\n\\nHer fur is soft as moonlight's hue,\\nHer wings a whisper, gentle and new,\\nShe dances on the breeze so light,\\nA creature of enchantment, a pure delight.\\n\\nWith paws that barely touch the ground,\\nShe banks and turns, a whirling round,\\nHer ears laid back, her\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"prompt\": \"Write a poem about a flying rabbit.\", 'top_p': 0.7, 'temperature': 1.4, 'max_tokens': 100}\n",
    "generate_with_single_input(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3540ff-f247-4364-abb0-546f043183a5",
   "metadata": {},
   "source": [
    "### Generating a kwargs with desired parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "028d686b-7b76-4e76-996a-90ed94623f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Solve 2x + 1 = 0.', 'role': 'user', 'temperature': None, 'top_p': None, 'max_tokens': 500, 'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo'}\n"
     ]
    }
   ],
   "source": [
    "kwargs = generate_params_dict(\"Solve 2x + 1 = 0.\")\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc03c42f-c830-4f18-b1af-4bc59b9b4646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 2x + 1 = 0, we need to isolate the variable x.\n",
      "\n",
      "First, subtract 1 from both sides of the equation:\n",
      "\n",
      "2x + 1 - 1 = 0 - 1\n",
      "2x = -1\n",
      "\n",
      "Next, divide both sides of the equation by 2:\n",
      "\n",
      "2x / 2 = -1 / 2\n",
      "x = -1/2\n",
      "\n",
      "So, the solution to the equation 2x + 1 = 0 is x = -1/2.\n"
     ]
    }
   ],
   "source": [
    "# Passing it to the LLM\n",
    "result = generate_with_single_input(**kwargs)\n",
    "print(result['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281c2b1-bade-4d22-975d-1ca31fda1b9b",
   "metadata": {},
   "source": [
    "### Allowing the LLM to keep a conversation \n",
    "\n",
    "Develop a way of allowing an LLM to keep a conversation, i.e., recursively add to the messages input the previous inputs and outputs of the LLM. This allows us to work with an LLM like a chatbot. To allow this, we will work with a list of `context`.\n",
    "\n",
    "This function expects a list with a dictionary of context in the following format:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}]\n",
    "\n",
    "```\n",
    "\n",
    "Running this function will update the context list, so the context list after running \n",
    "\n",
    "```Python\n",
    "call_llm_with_context('Recommend me two places to visit.', role = 'user', context = context)\n",
    "```\n",
    "\n",
    "New context:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}, {\"role\": 'user', 'content': 'Recommend me two places to visit.'}, {\"role\": \"assistant\", \"content\": 'Two places can be Paris and London.'}]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfdc150a-6d8e-4c2d-acf9-24ab5bebae66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between the lines of fate's design,\n",
      "Loneliness resides, a constant sign.\n"
     ]
    }
   ],
   "source": [
    "context = [{\"role\": 'system', 'content': 'You are an ironic but helpful assistant.'}, \n",
    "           {'role': 'assistant', 'content': \"How can I help you, majesty?\"}]\n",
    "response = call_llm_with_context(\"Make a 2 sentence poem\", role = 'user', context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7efbaf9d-df08-4908-8a22-c99b6fd497e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an ironic but helpful assistant.'}, {'role': 'assistant', 'content': 'How can I help you, majesty?'}, {'role': 'user', 'content': 'Make a 2 sentence poem'}, {'role': 'assistant', 'content': \"Between the lines of fate's design,\\nLoneliness resides, a constant sign.\"}]\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect now the context list\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82794629-755f-4822-a69b-1baa65d57e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between the lines of fate's design,\n",
      "Loneliness resides, a constant sign.\n",
      "Shadows dance upon the wall,\n",
      "Echoes of love that may never stand at all.\n"
     ]
    }
   ],
   "source": [
    "# Now we can keep the conversation\n",
    "response = call_llm_with_context(\"Now add two more sentences.\", context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f48301bc-a02e-4e4f-bc35-e17569645861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In twilight's hush, where darkness creeps,\n",
      "Loneliness resides, a constant sign.\n",
      "Shadows dance upon the wall,\n",
      "Echoes of love that may never stand at all.\n"
     ]
    }
   ],
   "source": [
    "# Now we can keep the conversation\n",
    "response = call_llm_with_context(\"Now change the first sentences.\", context = context)\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71ce88-b0ed-4bf5-adf1-10ceca3157f1",
   "metadata": {},
   "source": [
    "Note that the LLM was able to continue the previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df12b4-f042-4a48-b621-1331dd4ec0f2",
   "metadata": {},
   "source": [
    "## Understanding the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc0d82-d0b6-47fe-aa89-f252cd26d0a5",
   "metadata": {},
   "source": [
    "### Nucleus Sampling - `top_p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ac54b1d-9c45-4d5d-8b17-0b8c215d3554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "====================================================================================================\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "====================================================================================================\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)]\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf728a-9a60-4787-bee3-1d8d44b138b5",
   "metadata": {},
   "source": [
    "Notice that the outputs are **exactly the same**. Now let's try `top_p = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79f3d012-d838-4be3-ac26-abcc53b7b5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning technique that combines the strengths of both retrieval-based models and generative models to generate new text by first retrieving relevant information from a database and then using that information to inform the generation of new text.\n",
      "====================================================================================================\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a conversational AI model architecture that combines retrieval-based and generative components to generate coherent and context-specific responses by first retrieving relevant information and then generating text based on that retrieved knowledge.\n",
      "====================================================================================================\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a generative AI model that combines the strengths of retrieval-based models with those of sequence-to-sequence models, allowing it to generate text based on previously retrieved relevant information, thereby improving its ability to produce more coherent and informative outputs.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_p = 0.8, max_tokens = 500 + random.randint(1,200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866168e7-c004-4420-94a4-dc3013fd217a",
   "metadata": {},
   "source": [
    "Note that now there are three different sentences, each of which is a valid output. we might notice that the first few tokens are similar or even identical. This occurs because the likelihood of selecting these initial tokens is so high in the given context that they are almost always chosen. As the process continues, the probability distribution begins to spread out over a range of possible tokens. Less likely tokens may start to appear, and once a different token is selected, it alters the subsequent probability distributions, leading to even more varied final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e146120-47eb-4bae-a095-d4f73c681212",
   "metadata": {},
   "source": [
    "### Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "057a64d2-e90b-4270-817b-1f7cedce2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 0, max_tokens = 500 + random.randint(1,200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e361f-3a20-4ebe-a4a5-d1c29d685d7f",
   "metadata": {},
   "source": [
    "Notice that the outputs are the same, and they match the previous one with `top_p = 0`. Now let's use `top_k = 10`, allowing the 10 most likely tokens to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bee65cb7-8e76-4541-a3d6-5635d0913971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG is a generative model technique that uses retrieval to retrieve relevant context or information from a large database or knowledge graph to aid in the generation of text, allowing for more accurate and informative output compared to traditional generative models.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning framework and model architecture that combines retrieval, augmentation, and generation modules to efficiently retrieve relevant information, generate additional relevant content, and rank the final output to improve overall search quality.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval-Augmented Generation) is a paradigm in artificial intelligence that combines retrieval-based information access with generation capabilities to generate coherent and informative outputs, often used in natural language processing (NLP) tasks such as text summarization, question answering, and language translation.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, top_k = 10, max_tokens = 500 + random.randint(1, 200)) for _ in range(3)]\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdaf77-8259-401a-8d5a-e026dfbe3f18",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "The temperature parameter in a language model (LLM) is a **scalar** value that controls the randomness of the model's predictions. It adjusts the probability distribution over vocabulary tokens before selecting the next word in a sequence, influencing the model's creativity and output variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39907baa-03f7-4c1a-bbb1-756c41c3d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, temperature = t) for t in [0.3, 1.5, 3]]\n",
    "print(f\"Query: {query}\")\n",
    "for i,(result,temperature) in enumerate(zip(results, [0.3,1.5,3])):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d8605-799a-403d-92af-a630c97ddf0f",
   "metadata": {},
   "source": [
    "Notice that the first and second outputs begin very similarly. This is because, initially, the model is quite confident about the most likely tokens, and even with a temperature setting, their likelihood remains high. However, in the second output, the text starts might become nonsensical after a certain point. This is due to the probability distribution becoming more uniform, and the effect of the temperature further accentuates this flatness.\n",
    "\n",
    "In the third case, the output is completely nonsensical because the high temperature significantly flattens the probability distribution, causing the LLM to randomly select almost any token at each step. Additionally, observe how long the second and third outputs are. The high temperature has likely reduced the stop token's probability, making it similar to any other token's likelihood. Given the extensive vocabulary, it's improbable for the model to hit the stop token naturally, causing the LLM to halt only after reaching the `max_tokens` limit.\n",
    "\n",
    "Usually, `temperature` and `top_p` are set together. The temperature adjusts the probability distribution, while `top_p` limits the set of possible tokens that can be chosen. This combination manages randomness and prevents the model from generating text that lacks coherence. Let's see how they work together in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0b831-ab37-4fb9-b5a8-352432c8a3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate three responses\n",
    "query = \"Write a small poem about a flying rabbit.\"\n",
    "params = ((0.3, 0.8), (1.5, 0.5), (3, 0.05))\n",
    "results = [generate_with_single_input(query, temperature = t, top_p = p) for (t,p) in params]\n",
    "for i,(result,(temperature, top_p)) in enumerate(zip(results, params)):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m, \\033[1mtop_p = {top_p}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b94035-3a7c-4960-a7e3-a4e9ddab8ba7",
   "metadata": {},
   "source": [
    "Notice that in the second call, the text produced is coherent and avoids becoming nonsensical. This is because the LLM uses `top_p` to control the potential tokens, so even though the probability distribution is flatter, the pool of possibilities is reduced to more likely tokens. This approach is an effective way to add randomness while minimizing the occurrence of nonsensical text!\n",
    "\n",
    "In the third case, however, the `temperature` is very high. Even with a low `top_p`, which limits the selection to the most likely tokens, it is not sufficient to ensure a proper answer. Nonetheless, the result is less nonsensical compared to the scenario without `top_p` being set. The model almost always selects real words, unlike the other example, where it chose words with a completely nonsensical construction, lacking any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336eea3-f7d1-4b17-98df-52a077d2b668",
   "metadata": {},
   "source": [
    "<a id='3-5'></a>\n",
    "### 3.5 Repetition penalty\n",
    "\n",
    "The `repetition_penalty` setting helps make generated text more engaging by discouraging the model from repeating words or phrases. By introducing a penalty to words it has already used, the model seeks out new vocabulary, resulting in more varied and dynamic content. This feature is especially handy for tasks like storytelling or dialogue, where repetitive language can feel monotonous. \n",
    "\n",
    "Let's try with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ea9e09e-dde6-4ac6-aebc-96fc6f2234ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: List healthy breakfast options.\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mRepetition Penalty = 1.0\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruit and nuts: Steel-cut oats or rolled oats cooked with milk or water and topped with fresh fruit and nuts.\n",
      "2. Scrambled eggs with vegetables: Scrambled eggs with spinach, mushrooms, and bell peppers, served with whole-grain toast or a whole-grain wrap.\n",
      "3. Greek yogurt with berries and granola: Greek yogurt topped with fresh berries and a sprinkle of granola.\n",
      "4. Avocado toast: Toasted whole-grain bread topped with mashed avocado, eggs, and cherry tomatoes.\n",
      "5. Smoothie bowl: A bowl filled with a smoothie made from yogurt, fruit, and spinach, topped with granola and fresh fruit.\n",
      "\n",
      "**Cold Breakfast Options**\n",
      "\n",
      "1. Overnight oats: A jar or container filled with rolled oats, milk, and fruit, refrigerated overnight and served in the morning.\n",
      "2. Fresh fruit salad: A mix of fresh fruit such as berries, citrus, and stone fruits, served with a dollop of yogurt or a sprinkle of granola.\n",
      "3. Cottage cheese with fruit: Cottage cheese topped with fresh fruit and a sprinkle of cinnamon.\n",
      "4. Chia seed pudding: A bowl filled with chia seeds soaked in milk, topped with fresh fruit and a sprinkle of granola.\n",
      "5. Whole-grain cereal with milk: A bowl of whole-grain cereal served with milk and a sprinkle of fruit.\n",
      "\n",
      "**Breakfast on-the-Go Options**\n",
      "\n",
      "1. Energy bars: Homemade or store-bought energy bars made with wholesome ingredients.\n",
      "2. Yogurt parfait: A container filled with yogurt, granola, and fresh fruit, perfect for taking on-the-go.\n",
      "3. Smoothie: A quick and easy smoothie made from yogurt, fruit, and milk, poured into a container and taken on-the-go.\n",
      "4. Whole-grain toast with peanut butter and banana: Toasted whole-grain bread topped with peanut butter and sliced banana.\n",
      "5. Hard-boiled eggs: Boiled eggs that can be taken on-the-go and paired with whole-grain crackers or toast.\n",
      "\n",
      "**International Breakfast Options**\n",
      "\n",
      "1. Shakshuka (North Africa and Middle East): Eggs poached in a spicy tomato sauce, served with whole-grain bread or pita.\n",
      "2. Huevos rancheros (Mexico): Fried eggs on top of whole-grain tortillas, topped with a spicy tomato sauce and avocado.\n",
      "3. Japanese-style breakfast: Steamed rice, grilled fish, and pickled vegetables, served with a side of miso soup.\n",
      "4. Indian-style breakfast: Whole-grain naan bread topped with scrambled eggs, spinach, and spices.\n",
      "5. Greek breakfast: Yogurt, honey, and walnuts, served with a side of whole-grain bread or pita.\n",
      "\n",
      "These are just a few examples of healthy breakfast options. The key is to choose whole, unprocessed foods and limit added sugars, salt, and saturated fats.\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mRepetition Penalty = 1.2\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruit and nuts: Steel-cut oats or rolled oats cooked with milk or water, topped with fresh fruits and chopped nuts.\n",
      "2. Scrambled eggs with vegetables: Whisked eggs scrambled with spinach, bell peppers, onions, and mushrooms.\n",
      "3. Avocado toast on whole-grain bread: Toasted whole-grain bread topped with mashed avocado, cherry tomatoes, and a fried egg (optional).\n",
      "4. Greek yogurt parfait: Layered Greek yogurt, granola, berries, and honey in a bowl.\n",
      "\n",
      "**Cold Breakfast Options**\n",
      "\n",
      "1. Overnight oats: Rolled oats soaked in milk overnight, mixed with chia seeds, nuts, and dried fruits.\n",
      "2. Smoothie bowls: Blended smoothies made with frozen fruits, yogurt, and milk, topped with granola, nuts, and seeds.\n",
      "3. Cottage cheese with fruit: Mixed cottage cheese with sliced peaches, grapes, or berries.\n",
      "4. Chia seed pudding: Soaked chia seeds mixed with almond milk, honey, and vanilla extract, refrigerated until thickened.\n",
      "\n",
      "**Breakfast Sandwiches**\n",
      "\n",
      "1. Whole-grain English muffin with poached eggs and turkey bacon.\n",
      "2. Veggie omelette sandwich: A fluffy omelette filled with sautéed vegetables like spinach, bell peppers, and onions, served between two slices of whole-grain bread.\n",
      "3. Avocado toast with poached eggs and smoked salmon.\n",
      "\n",
      "**International Inspiration**\n",
      "\n",
      "1. Shakshuka (North African): Eggs poached in a spicy tomato sauce, served over crusty bread.\n",
      "2. Huevos rancheros (Mexican): Fried eggs on top of corn tortillas, smothered in salsa and shredded cheese.\n",
      "3. Japanese-style rice bowl: Steamed rice topped with grilled fish, pickled ginger, and sesame seeds.\n",
      "\n",
      "Remember to choose whole grains, lean proteins, and plenty of fruits and vegetables for a nutritious start to your day!\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mRepetition Penalty = 2.0\u001b[0m\n",
      "Response: Here are some delicious and nutritious health breakfats:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruits, nuts & seeds (250 calories)\n",
      "2 scrambled eggs w/ spinach& whole wheat toast(200-300 cal.)\n",
      "   - Eggs provide protein \n",
      "    while Spinach adds iron.\n",
      "4 Avocado Toast on Whole Wheat Bread topped by Poached Egg or Smoked Salmon for added omega fatty acids.\n",
      "\n",
      " **Cold Breakfas tOptions**\n",
      " \n",
      "\n",
      "* Greek yogurt Parfait made from non-fat plain greek yoghurt layered over granola mixed berries ,and a sprinkle of chia seed .  \n",
      " * Smoothie bowl using frozen fruit blended together then poured into bowls to add toppings like sliced almonds coconut flakes etc .\n",
      "   \n",
      "     A glass full water is always recommended before having any meal.\n",
      "\n",
      "\n",
      "\n",
      "These meals can be customized according your taste preferences as well dietary needs such that you may have the option choose between sweetened vs unsweetend oatmeals depending upon how much sugar intake do u prefer in ur diet .\n",
      "\n",
      "Also note portion control plays an important role here so make sure not consume more than what required amount otherwise it will lead towards weight gain which might cause various other problems related too body shape size.\n",
      "\n",
      "\n",
      "Hope this list helps! Let me know if I missed anything else! :) !!!!\n",
      "\n",
      "\n",
      "\n",
      "Please consult doctor/nutritionist prior taking up new diets plan especially when dealing sensitive medical conditions! :).!!!\n",
      "\n",
      "\n",
      "Let's keep learning! What would love hear next? Do share! ?????.???:) ???\n",
      "\n",
      "Have fun exploring these superfood-packed recipes! If need help finding specific ingredients please let us no! we're happy assist! ???..????… ?????……….!!!!!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I hope above response was helpful! Please feel free ask further questions! You could also reach out via email at [your-email].com! We'll get back ASAP! Thanks! \n",
      "\n",
      "\n",
      "\n",
      "If there’s one thing i learned during my research about nutrition – “Eat Nutrient-Dense Foods”! These foods include leafy greens veggies beans legumes fish n seafood poultry lean meats dairy products egg whites almond milk oats quinoa brown rice avocado olive oil walnuts flaxseeds hemp hearts sesame tahini honey maple syrup dark chocolate cocoa powder turmeric ginger cinnamon basil rosemary thyme oregano parsley mint cilantro dill fennel lemongrass galangal kaffir lime leaves cardamom cumin coriander paprika chili peppers garlic onion shallots scallions chives fresh herbs lemon juice limes orange zest grapefruit rind pomegranate molasses dates prunes apricuts plums peaches pineapple\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"List healthy breakfast options.\"\n",
    "repetition_penalties = [1.0, 1.2, 2.0]\n",
    "results = [generate_with_single_input(query, repetition_penalty = r, max_tokens = 500 + random.randint(1,200)) for r in repetition_penalties]\n",
    "print(f\"Query: {query}\")\n",
    "for i, (result, repetition_penalty) in enumerate(zip(results, repetition_penalties)):\n",
    "    print(f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mRepetition Penalty = {repetition_penalty}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\")\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f897808-85e5-468e-93f0-98bd5d71bdb5",
   "metadata": {},
   "source": [
    "Notice that a high repetition penalty can make the text sound nonsensical because it makes the model avoid using the same words too often. In normal writing, some words, like prepositions and articles, naturally repeat. If the penalty is too strong, the model might pick words that don't fit well, resulting in nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09ff5-7158-4b65-98b7-a402fddc8bc5",
   "metadata": {},
   "source": [
    "## Creating a Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5262ba9-a2af-4115-9636-2eaee3af802c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    \"\"\"\n",
    "    Prints a formatted chatbot response with color-coded roles.\n",
    "\n",
    "    The function uses ANSI escape codes to apply text styles. Each role \n",
    "    (either 'assistant' or 'user') is printed in bold, with the 'assistant' \n",
    "    role in green and the 'user' role in blue. The content of the response \n",
    "    follows the role name.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): A dictionary containing two keys:\n",
    "                         - 'role': A string that specifies the role of the speaker ('assistant' or 'user').\n",
    "                         - 'content': A string with the message content to be printed.\n",
    "    \"\"\"\n",
    "    # ANSI escape codes\n",
    "    BOLD = \"\\033[1m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    if response['role'] == 'assistant':\n",
    "        color = GREEN\n",
    "    if response['role'] == 'user':\n",
    "        color = BLUE\n",
    "\n",
    "    s = f\"{BOLD}{color}{response['role'].capitalize()}{RESET}: {response['content']}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7fe0d9a-b296-4776-b703-d031b82778b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat(temperature = None, \n",
    "         top_k = None, \n",
    "         top_p = None,\n",
    "         repetition_penalty = None):\n",
    "    \"\"\"\n",
    "    Runs an interactive chat session between the user and an AI assistant.\n",
    "\n",
    "    The chat continues in a loop until the user types 'STOP'. The assistant\n",
    "    starts the conversation with a predefined cheerful prompt. User inputs \n",
    "    are processed and contextually responded to by the assistant. Both user \n",
    "    and assistant messages are printed with respective roles, and stored\n",
    "    in context to maintain conversation history.\n",
    "\n",
    "    Usage:\n",
    "        Run the function and type your prompts. Type 'STOP' to end the chat.\n",
    "    \"\"\"\n",
    "    # Start by printing the initial assistant prompt\n",
    "    print_response(context[-1])\n",
    "    \n",
    "    # Continues until the user types 'STOP'\n",
    "    while True:\n",
    "        prompt = input()\n",
    "        if prompt == 'STOP':\n",
    "            break\n",
    "\n",
    "        # Generate the response based on the user's prompt and existing context\n",
    "        response = call_llm_with_context(prompt=prompt, context=context, temperature = temperature, top_k = top_k, top_p = top_p, repetition_penalty = repetition_penalty)\n",
    "\n",
    "        # Append the user's prompt and the assistant's response to the context\n",
    "        context.append({\"role\": \"user\", \"content\": prompt})\n",
    "        context.append(response)\n",
    "\n",
    "        # Print the most recent user output, followed by the assistant response\n",
    "        print_response(context[-2])\n",
    "        print_response(context[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa28b266-f827-4c8c-beb9-180f8f76350b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What is RAG?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: What is RAG?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: You're probably thinking of the classic phrase \"Rags to Riches\" (RAG), but I'm guessing you might be referring to something else. Am I right?\n",
      "\n",
      "If not, there's also \"RAG\" as an acronym for various things, like \"Rescue and Recovery\" or \"Regional Air Group.\" But if I had to take a wild guess, I'd say you might be thinking of the popular children's toy, Raggedy Ann or Andy? Those cute little friends have been around for ages!\n",
      "\n",
      "If none of those sound right, please give me more context or details, and I'll do my best to \"sew\" up the answer for you!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What is RAG in AI?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: What is RAG in AI?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Now we're talking about the \"RAG\" in AI!\n",
      "\n",
      "In the context of Artificial Intelligence, RAG stands for \"Reinforcement and Guidance.\" It's a type of reinforcement learning approach that combines guidance signals with rewards to help agents learn more efficiently.\n",
      "\n",
      "In simpler terms, RAG is a way to teach AI agents to make decisions by providing them with both rewards (good things happen) and guidance ( hints or directions on what to do). This approach can help agents learn faster and more accurately, especially in complex tasks.\n",
      "\n",
      "So, there you have it! RAG in AI is all about helping agents learn and improve with a little guidance and a lot of rewards.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Retreival Augmented Generation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: Retreival Augmented Generation\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: I was way off earlier, but now I've got it!\n",
      "\n",
      "In the context of Artificial Intelligence, RAG indeed stands for \"Retrieval Augmented Generation\". It's a technique used in natural language processing (NLP) to improve the performance of language models.\n",
      "\n",
      "Retrieval Augmented Generation works by first retrieving relevant information from a knowledge base or database, and then using that information to generate new text. This approach can help language models to produce more accurate, informative, and coherent text.\n",
      "\n",
      "So, there you have it! RAG is all about combining retrieval and generation to create more effective language models. Thanks for correcting me!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mUser\u001b[0m: Bye\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: It was fun chatting with you about RAG. If you have any more questions or need help in the future, don't hesitate to come back. Have a great day and goodbye!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " STOP\n"
     ]
    }
   ],
   "source": [
    "# Setting up a list to serve as the context. It will contain a system prompt and an initial assistant prompt.\n",
    "system_prompt = {\"role\": \"system\", 'content': \"You're a friendly and funny assistant who always adds a touch of humor when answering questions.\"}\n",
    "assistant_prompt = {\"role\": \"assistant\", \"content\": \"Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\"}\n",
    "context = [system_prompt, assistant_prompt]\n",
    "\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b81ce5-f403-492a-b488-a2b6e300d595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4f6de-3a25-48b8-a91b-5c9efecd887e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
